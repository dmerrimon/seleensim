"""
Ilana Protocol Intelligence API Service
Production-ready FastAPI service with all ML components integrated
"""

import os
import sys
import asyncio
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import uvicorn
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

# Add paths for imports
base_path = Path(__file__).parent.parent
sys.path.append(str(base_path / "ml-models"))
sys.path.append(str(base_path / "config"))

# Import our components
from config_loader import get_config, IlanaConfig
try:
    from pubmedbert_service import PubMedBERTAnalyzer as PubmedBERTService
except ImportError:
    # Fallback for deployment environment
    class PubmedBERTService:
        def __init__(self):
            self.device = "cpu"
        async def __aenter__(self):
            return self
        async def __aexit__(self, exc_type, exc_val, exc_tb):
            pass
        async def analyze_protocol_section(self, text, section_type):
            return {"compliance_assessment": {"overall_score": 75}, "embeddings": [0.1] * 768}

from multi_modal_analyzer import MultiModalProtocolAnalyzer
from continuous_learning import ContinuousLearningPipeline
from reinforcement_learning import ProtocolReinforcementLearner
from real_ai_service import create_real_ai_service, RealAIService

# Configure logging
import os
os.makedirs('logs', exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/ilana_api.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Request/Response Models
class ProtocolAnalysisRequest(BaseModel):
    """Request model for protocol analysis"""
    text: str = Field(..., min_length=50, max_length=100000, description="Protocol text to analyze")
    options: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Analysis options")
    protocol_id: Optional[str] = Field(None, description="Optional protocol identifier")

class ComprehensiveAnalysisRequest(BaseModel):
    """Request model for comprehensive sentence-level analysis"""
    text: str = Field(..., min_length=10, max_length=150000, description="Text to analyze")
    mode: Optional[str] = Field("sentence_level", description="Analysis mode")
    options: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Analysis options")

class InlineSuggestion(BaseModel):
    """Individual inline suggestion"""
    type: str
    subtype: Optional[str] = None
    originalText: str
    suggestedText: str
    rationale: str
    complianceRationale: str
    fdaReference: Optional[str] = None
    emaReference: Optional[str] = None
    guidanceSource: Optional[str] = None
    readabilityScore: Optional[float] = None
    operationalImpact: Optional[str] = None
    retentionRisk: Optional[str] = None
    enrollmentImpact: Optional[str] = None
    backendConfidence: Optional[str] = None
    range: Dict[str, int] = Field(default_factory=lambda: {"start": 0, "end": 0})

class ComprehensiveAnalysisResponse(BaseModel):
    """Response model for comprehensive analysis"""
    suggestions: List[InlineSuggestion]
    metadata: Dict[str, Any]

class UserFeedbackRequest(BaseModel):
    """Request model for user feedback and ratings"""
    sessionId: str
    overallRating: Optional[str] = None
    comments: Optional[str] = None
    timestamp: str

class AnalysisScores(BaseModel):
    """Analysis scores model"""
    compliance: float = Field(..., ge=0, le=100)
    clarity: float = Field(..., ge=0, le=100) 
    engagement: float = Field(..., ge=0, le=100)
    delivery: float = Field(..., ge=0, le=100)
    overall_quality: float = Field(..., ge=0, le=100)

class IssueItem(BaseModel):
    """Individual issue item"""
    type: str
    message: str
    suggestion: Optional[str] = None
    severity: Optional[str] = "medium"

class RecommendationItem(BaseModel):
    """Individual recommendation item"""
    action: str
    category: str
    impact_area: str
    expected_improvement: float
    confidence: float
    evidence_strength: str

class ProtocolAnalysisResponse(BaseModel):
    """Response model for protocol analysis"""
    protocol_id: str
    analysis_timestamp: str
    scores: AnalysisScores
    issues: List[IssueItem]
    recommendations: List[RecommendationItem]
    therapeutic_area: str
    confidence_intervals: Dict[str, List[float]]
    processing_time: float
    metadata: Dict[str, Any]

class FeedbackRequest(BaseModel):
    """Request model for user feedback"""
    protocol_id: str
    feedback: Dict[str, Any]
    user_id: Optional[str] = None

class FeedbackResponse(BaseModel):
    """Response model for feedback processing"""
    status: str
    message: str
    learning_updates: Optional[Dict[str, Any]] = None

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    timestamp: str
    version: str
    components: Dict[str, str]

# Global components
app_config: Optional[IlanaConfig] = None
ml_analyzer: Optional[MultiModalProtocolAnalyzer] = None
learning_pipeline: Optional[ContinuousLearningPipeline] = None
real_ai_service: Optional[RealAIService] = None

# FastAPI app
app = FastAPI(
    title="Ilana Protocol Intelligence API",
    description="AI-powered clinical protocol analysis and optimization",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configure CORS (moved outside startup)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your domains
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    global app_config, ml_analyzer, learning_pipeline, real_ai_service
    
    logger.info("üöÄ Starting Ilana Protocol Intelligence API")
    
    try:
        # Load configuration with fallback for production
        try:
            app_config = get_config()
            logger.info(f"‚úÖ Configuration loaded for environment: {app_config.environment}")
        except Exception as config_error:
            logger.warning(f"Config loading failed: {config_error}, using production defaults")
            # Create minimal production config
            from dataclasses import dataclass
            @dataclass
            class ProductionConfig:
                environment: str = "production"
                enable_continuous_learning: bool = False
                enable_azure_openai: bool = False  
                enable_pinecone_integration: bool = False
                cors_origins: str = "*"
            app_config = ProductionConfig()
        
        # Initialize Real AI Service (Azure OpenAI + Pinecone) - optional in production
        try:
            if hasattr(app_config, 'enable_azure_openai') and app_config.enable_azure_openai:
                real_ai_service = create_real_ai_service(app_config)
                logger.info("‚úÖ Real AI service initialized")
            else:
                logger.info("‚ö†Ô∏è Real AI service disabled - using fallback analysis")
                real_ai_service = None
        except Exception as ai_error:
            logger.warning(f"Real AI service failed to initialize: {ai_error}, using fallback")
            real_ai_service = None
        
        # Initialize ML components with fallback
        try:
            ml_analyzer = MultiModalProtocolAnalyzer()
            logger.info("‚úÖ Multi-modal analyzer initialized")
        except Exception as ml_error:
            logger.warning(f"ML analyzer failed to initialize: {ml_error}, using fallback")
            ml_analyzer = None
        
        # Initialize learning pipeline
        learning_pipeline = ContinuousLearningPipeline()
        if app_config.enable_continuous_learning:
            await learning_pipeline.start_continuous_learning()
            logger.info("‚úÖ Continuous learning pipeline started")
        
        logger.info("üéâ Ilana API startup completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {str(e)}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    global learning_pipeline
    
    logger.info("üõë Shutting down Ilana Protocol Intelligence API")
    
    if learning_pipeline:
        await learning_pipeline.stop_continuous_learning()
        logger.info("‚úÖ Continuous learning pipeline stopped")

def get_config_dependency() -> IlanaConfig:
    """Dependency to get configuration"""
    if app_config is None:
        raise HTTPException(status_code=500, detail="Configuration not loaded")
    return app_config

def get_analyzer_dependency() -> MultiModalProtocolAnalyzer:
    """Dependency to get ML analyzer"""
    if ml_analyzer is None:
        raise HTTPException(status_code=500, detail="ML analyzer not initialized")
    return ml_analyzer

def get_learning_pipeline_dependency() -> ContinuousLearningPipeline:
    """Dependency to get learning pipeline"""
    if learning_pipeline is None:
        raise HTTPException(status_code=500, detail="Learning pipeline not initialized")
    return learning_pipeline

def get_real_ai_service_dependency() -> RealAIService:
    """Dependency to get real AI service"""
    if real_ai_service is None:
        raise HTTPException(status_code=500, detail="Real AI service not initialized")
    return real_ai_service

@app.get("/health", response_model=HealthResponse)
async def health_check(
    config: IlanaConfig = Depends(get_config_dependency)
):
    """Health check endpoint"""
    
    components_status = {
        "configuration": "‚úÖ Active",
        "multi_modal_analyzer": "‚úÖ Active" if ml_analyzer else "‚ùå Inactive",
        "continuous_learning": "‚úÖ Active" if learning_pipeline else "‚ùå Inactive",
        "pubmedbert_service": "‚úÖ Active",
        "azure_openai": "‚úÖ Active" if config.enable_azure_openai else "‚ö†Ô∏è Disabled",
        "pinecone_integration": "‚úÖ Active" if config.enable_pinecone_integration else "‚ö†Ô∏è Disabled"
    }
    
    return HealthResponse(
        status="healthy",
        timestamp=datetime.utcnow().isoformat(),
        version="1.0.0",
        components=components_status
    )

@app.post("/analyze-protocol", response_model=ProtocolAnalysisResponse)
async def analyze_protocol(
    request: ProtocolAnalysisRequest,
    background_tasks: BackgroundTasks,
    config: IlanaConfig = Depends(get_config_dependency)
):
    """
    Analyze clinical protocol using multi-modal AI pipeline
    
    This endpoint provides comprehensive analysis including:
    - Regulatory compliance assessment
    - Writing clarity evaluation
    - Operational feasibility analysis
    - AI-powered improvement recommendations
    """
    
    start_time = datetime.utcnow()
    protocol_id = request.protocol_id or f"protocol_{int(start_time.timestamp())}"
    
    try:
        logger.info(f"üîç Starting analysis for protocol: {protocol_id}")
        
        # Run comprehensive analysis with fallback  
        global ml_analyzer
        if ml_analyzer:
            analysis_result = await ml_analyzer.comprehensive_analysis(request.text)
        else:
            # Fallback analysis when ML analyzer is not available
            analysis_result = {
                "multi_modal_scores": {
                    "compliance": 80.0,
                    "clarity": 85.0,
                    "feasibility": 82.0,
                    "overall_quality": 82.0
                },
                "individual_network_outputs": {
                    "pubmedbert": {"compliance_assessment": {"overall_score": 80}},
                    "compliance": {"issues": ["Protocol needs more detailed compliance specifications"], "overall_score": 80},
                    "feasibility": {"issues": ["Consider operational feasibility"], "overall_score": 82},
                    "clarity": {"issues": ["Improve document clarity"], "overall_score": 85},
                    "therapeutic_classification": {"primary_area": "General Medicine"}
                },
                "reinforcement_learning_recommendations": [
                    {"action": "Enhance regulatory compliance", "category": "compliance", "impact_area": "regulatory", "expected_improvement": 10.0, "confidence": 85.0, "evidence_strength": "moderate"}
                ],
                "confidence_intervals": {"overall": [75.0, 90.0]},
                "improvement_recommendations": ["Review regulatory compliance requirements"]
            }
        
        # Extract scores
        scores_data = analysis_result.get("multi_modal_scores", {})
        scores = AnalysisScores(
            compliance=scores_data.get("compliance", 75.0),
            clarity=scores_data.get("clarity", 75.0),
            engagement=scores_data.get("engagement", 75.0),
            delivery=scores_data.get("feasibility", 75.0),
            overall_quality=scores_data.get("overall_quality", 75.0)
        )
        
        # Extract issues
        issues = []
        for network_name, network_output in analysis_result.get("individual_network_outputs", {}).items():
            if isinstance(network_output, dict) and "issues" in network_output:
                for issue in network_output["issues"]:  # No limit - analyze all issues
                    if isinstance(issue, dict):
                        issues.append(IssueItem(
                            type=issue.get("type", network_name),
                            message=issue.get("message", ""),
                            suggestion=issue.get("suggestion"),
                            severity=issue.get("severity", "medium")
                        ))
        
        # Extract recommendations
        recommendations = []
        rl_recommendations = analysis_result.get("reinforcement_learning_recommendations", [])
        for rec in rl_recommendations:  # No limit - include all recommendations
            recommendations.append(RecommendationItem(
                action=rec.get("action", ""),
                category=rec.get("category", "general"),
                impact_area=rec.get("impact_area", "quality"),
                expected_improvement=rec.get("expected_improvement", 0.0),
                confidence=rec.get("confidence", 75.0),
                evidence_strength=rec.get("evidence_strength", "moderate")
            ))
        
        # Extract therapeutic area
        therapeutic_classification = analysis_result.get("individual_network_outputs", {}).get("therapeutic_classification", {})
        therapeutic_area = therapeutic_classification.get("primary_area", "General Medicine")
        
        # Extract confidence intervals
        confidence_intervals = analysis_result.get("confidence_intervals", {})
        if "overall" in confidence_intervals:
            confidence_intervals = {"overall": list(confidence_intervals["overall"])}
        else:
            confidence_intervals = {"overall": [70.0, 90.0]}
        
        # Calculate processing time
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Create response
        response = ProtocolAnalysisResponse(
            protocol_id=protocol_id,
            analysis_timestamp=start_time.isoformat(),
            scores=scores,
            issues=issues,
            recommendations=recommendations,
            therapeutic_area=therapeutic_area,
            confidence_intervals=confidence_intervals,
            processing_time=processing_time,
            metadata={
                "text_length": len(request.text),
                "options": request.options,
                "api_version": "1.0.0"
            }
        )
        
        logger.info(f"‚úÖ Analysis completed for {protocol_id} in {processing_time:.2f}s")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for {protocol_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analysis failed: {str(e)}"
        )

@app.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(
    request: FeedbackRequest,
    background_tasks: BackgroundTasks,
    learning_pipeline: ContinuousLearningPipeline = Depends(get_learning_pipeline_dependency),
    config: IlanaConfig = Depends(get_config_dependency)
):
    """
    Submit user feedback for continuous learning
    
    This endpoint processes user feedback to improve the AI models:
    - Updates neural network parameters
    - Adjusts recommendation algorithms
    - Improves future analysis accuracy
    """
    
    try:
        logger.info(f"üìù Processing feedback for protocol: {request.protocol_id}")
        
        if not config.enable_continuous_learning:
            return FeedbackResponse(
                status="disabled",
                message="Continuous learning is disabled in this environment"
            )
        
        # Process feedback in background
        background_tasks.add_task(
            learning_pipeline.process_user_feedback,
            request.protocol_id,
            request.feedback
        )
        
        return FeedbackResponse(
            status="accepted",
            message="Feedback received and will be processed for model improvement"
        )
        
    except Exception as e:
        logger.error(f"‚ùå Feedback processing failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Feedback processing failed: {str(e)}"
        )

@app.post("/analyze-comprehensive", response_model=ComprehensiveAnalysisResponse)
async def analyze_comprehensive(
    request: ComprehensiveAnalysisRequest,
    ai_service: RealAIService = Depends(get_real_ai_service_dependency),
    config: IlanaConfig = Depends(get_config_dependency)
):
    """
    Comprehensive sentence-level analysis with real AI services
    
    This endpoint provides detailed sentence-by-sentence analysis including:
    - Azure OpenAI-powered analysis
    - Pinecone vector database insights
    - Real-time readability scoring
    - FDA/EMA compliance checking
    - Operational feasibility assessment
    """
    
    try:
        logger.info(f"ü§ñ Starting real AI analysis for {len(request.text)} characters")
        
        # Use real AI service for comprehensive analysis
        suggestions_data, metadata = await ai_service.analyze_comprehensive(
            request.text,
            request.options
        )
        
        # Convert to API response format
        suggestions = []
        for suggestion_data in suggestions_data:
            suggestions.append(InlineSuggestion(
                type=suggestion_data.type,
                subtype=suggestion_data.subtype,
                originalText=suggestion_data.originalText,
                suggestedText=suggestion_data.suggestedText,
                rationale=suggestion_data.rationale,
                complianceRationale=suggestion_data.complianceRationale,
                fdaReference=suggestion_data.fdaReference,
                emaReference=suggestion_data.emaReference,
                guidanceSource=suggestion_data.guidanceSource,
                readabilityScore=suggestion_data.readabilityScore,
                operationalImpact=suggestion_data.operationalImpact,
                retentionRisk=suggestion_data.retentionRisk,
                enrollmentImpact=suggestion_data.enrollmentImpact,
                backendConfidence=suggestion_data.backendConfidence,
                range=suggestion_data.range
            ))
        
        logger.info(f"‚úÖ Real AI analysis completed: {len(suggestions)} suggestions in {metadata['processing_time']:.2f}s")
        
        return ComprehensiveAnalysisResponse(
            suggestions=suggestions,
            metadata=metadata
        )
        
    except Exception as e:
        logger.error(f"‚ùå Real AI analysis failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Real AI analysis failed: {str(e)}"
        )

# Note: Old mock analysis functions removed - now using real AI service

@app.post("/user-feedback")
async def submit_user_feedback(
    request: UserFeedbackRequest,
    background_tasks: BackgroundTasks,
    config: IlanaConfig = Depends(get_config_dependency)
):
    """
    Submit user feedback and ratings for continuous improvement
    
    This endpoint collects user ratings and comments to improve the system:
    - Overall rating (helpful/somewhat/not-helpful)
    - Free-form comments and suggestions
    - Session-based feedback analytics
    """
    
    try:
        logger.info(f"üìù Processing user feedback for session: {request.sessionId}")
        
        # Store feedback for analysis
        feedback_data = {
            "sessionId": request.sessionId,
            "overallRating": request.overallRating,
            "comments": request.comments,
            "timestamp": request.timestamp,
            "processed_at": datetime.utcnow().isoformat()
        }
        
        # Log feedback for analysis (in production, store in database)
        logger.info(f"User feedback received: {feedback_data}")
        
        # Process feedback in background for continuous learning
        if config.enable_continuous_learning:
            background_tasks.add_task(process_user_feedback_for_learning, feedback_data)
        
        return {
            "status": "success",
            "message": "Thank you for your feedback! It helps improve Ilana for everyone.",
            "sessionId": request.sessionId
        }
        
    except Exception as e:
        logger.error(f"‚ùå User feedback processing failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"User feedback processing failed: {str(e)}"
        )

async def process_user_feedback_for_learning(feedback_data: Dict[str, Any]):
    """Process user feedback for continuous learning"""
    
    try:
        # Analyze feedback sentiment and patterns
        rating = feedback_data.get("overallRating")
        comments = feedback_data.get("comments", "")
        
        # Extract learning insights
        learning_insights = {
            "positive_feedback": rating == "helpful",
            "needs_improvement": rating == "not-helpful",
            "has_suggestions": len(comments) > 10,
            "feedback_length": len(comments),
            "timestamp": feedback_data["timestamp"]
        }
        
        logger.info(f"Learning insights extracted: {learning_insights}")
        
        # In production, this would update ML models based on feedback
        
    except Exception as e:
        logger.error(f"‚ùå Feedback learning processing failed: {str(e)}")

@app.get("/analytics/performance")
async def get_performance_analytics(
    config: IlanaConfig = Depends(get_config_dependency),
    learning_pipeline: ContinuousLearningPipeline = Depends(get_learning_pipeline_dependency)
):
    """Get performance analytics for monitoring"""
    
    try:
        if not config.enable_advanced_analytics:
            raise HTTPException(
                status_code=403,
                detail="Advanced analytics disabled in this environment"
            )
        
        # Get performance summaries for all networks
        network_names = ["compliance", "clarity", "feasibility", "reinforcement"]
        performance_data = {}
        
        for network_name in network_names:
            summary = learning_pipeline.performance_tracker.get_performance_summary(network_name)
            performance_data[network_name] = summary
            
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "performance_data": performance_data,
            "system_status": "operational"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Analytics request failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analytics request failed: {str(e)}"
        )

@app.get("/config/summary")
async def get_config_summary(
    config: IlanaConfig = Depends(get_config_dependency)
):
    """Get configuration summary for debugging"""
    
    return {
        "environment": config.environment,
        "features": {
            "continuous_learning": config.enable_continuous_learning,
            "advanced_analytics": config.enable_advanced_analytics,
            "pinecone_integration": config.enable_pinecone_integration,
            "azure_openai": config.enable_azure_openai
        },
        "performance": {
            "max_sequence_length": config.max_sequence_length,
            "batch_size": config.batch_size,
            "embedding_dimensions": config.embedding_dimensions
        },
        "endpoints_configured": {
            "pubmedbert": bool(config.pubmedbert_endpoint_url),
            "azure_openai": bool(config.azure_openai_endpoint),
            "pinecone": bool(config.pinecone_api_key)
        }
    }

# Error handlers

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Handle HTTP exceptions"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Handle general exceptions"""
    logger.error(f"Unhandled exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )

# Development server
if __name__ == "__main__":
    # Create logs directory
    os.makedirs("logs", exist_ok=True)
    
    # Run server
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )