#!/usr/bin/env python3
"""
Ilana Protocol Intelligence API Service - Enterprise Production Version
Full enterprise AI stack with Azure OpenAI + Pinecone + PubMedBERT
"""

import os
import sys
import uuid
import logging
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
env_path = Path(__file__).parent / "config" / "environments" / "production.env"
if env_path.exists():
    load_dotenv(env_path)
    logging.info(f"‚úÖ Loaded environment from {env_path}")
else:
    logging.warning(f"‚ö†Ô∏è Environment file not found at {env_path}")

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import httpx

# Add parent directory to path for enterprise components
parent_path = Path(__file__).parent.parent
sys.path.insert(0, str(parent_path))
sys.path.insert(0, str(parent_path / "config"))

# Import enterprise AI components
try:
    from config_loader import get_config, IlanaConfig
    from optimized_real_ai_service import create_optimized_real_ai_service, OptimizedRealAIService, InlineSuggestion
    ENTERPRISE_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ Enterprise AI components loaded successfully")
except ImportError as e:
    ENTERPRISE_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è Enterprise AI components not available: {e}")

# Fallback InlineSuggestion
from dataclasses import dataclass
@dataclass
class InlineSuggestion:
    type: str
    subtype: Optional[str] = None
    originalText: str = ""
    suggestedText: str = ""
    rationale: str = ""
    complianceRationale: str = ""
    fdaReference: Optional[str] = None
    emaReference: Optional[str] = None
    guidanceSource: Optional[str] = None
    readabilityScore: Optional[float] = None
    operationalImpact: Optional[str] = None
    retentionRisk: Optional[str] = None
    enrollmentImpact: Optional[str] = None
    backendConfidence: Optional[str] = None
    range: Dict[str, int] = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global enterprise AI service
enterprise_ai_service: Optional[OptimizedRealAIService] = None

# Request/Response Models  
class ComprehensiveAnalysisRequest(BaseModel):
    """Request model for comprehensive analysis"""
    text: str = Field(..., min_length=10, description="Text to analyze")
    options: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Analysis options")
    chunk_index: Optional[int] = Field(0, description="Chunk index for processing")
    total_chunks: Optional[int] = Field(1, description="Total number of chunks")

class TADetectRequest(BaseModel):
    """Request model for TA detection"""
    content: str

class TextEnhanceRequest(BaseModel):
    """Request model for text enhancement"""
    original_text: str
    therapeutic_area: Optional[str] = "general_medicine"
    enhancement_type: Optional[str] = "clarity_and_compliance"

class TARecommendationsRequest(BaseModel):
    """Request model for TA recommendations"""
    therapeutic_area: str
    protocol_type: Optional[str] = "clinical_trial"

# FastAPI app
app = FastAPI(
    title="Ilana Protocol Intelligence API",
    description="AI-powered clinical protocol analysis and optimization with real medical intelligence",
    version="1.3.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# ---- START PATCH: Analysis Mode Routing ----
# Ensure these environment variables exist; default to "simple"
ANALYSIS_MODE = os.getenv("ANALYSIS_MODE", "simple").lower()  # simple | hybrid | legacy
ANALYSIS_SERVICE_BASE = os.getenv("ANALYSIS_SERVICE_BASE", "http://127.0.0.1:8000")  # local fallback

analysis_logger = logging.getLogger("ilana.analysis_mode")
analysis_logger.setLevel(logging.INFO)

# Try to import existing simple endpoint handler; if not available, we'll call HTTP fallback
def _call_simple_inprocess(payload: dict):
    """
    Attempt to call an in-process simple handler if available.
    Expected signature in existing code (optional):
      def recommend_language_simple(payload: dict) -> dict
    """
    try:
        # adjust import path if your module name differs
        from recommend_simple import recommend_language_simple
        return recommend_language_simple(payload)
    except Exception:
        return None

def _call_legacy_inprocess(payload: dict):
    try:
        from legacy_pipeline import run_legacy_pipeline  # if exists
        return run_legacy_pipeline(payload)
    except Exception:
        return None

async def _http_post(path: str, payload: dict, timeout: int = 25):
    url = f"{ANALYSIS_SERVICE_BASE.rstrip('/')}/{path.lstrip('/')}"
    async with httpx.AsyncClient(timeout=timeout) as client:
        resp = await client.post(url, json=payload)
        resp.raise_for_status()
        # Return JSON if possible, else text
        try:
            return resp.json()
        except Exception:
            return {"raw": resp.text}

def _generate_request_id():
    return str(uuid.uuid4())

@app.post("/api/analyze")
async def analyze_entry(request: Request):
    """
    Unified analyze entry point that dispatches by ANALYSIS_MODE.
    Payload expected: {"text": "...", "mode":"selection"|"document", "ta": "...", "phase":"..."}
    Returns either immediate suggestions (list) or a job_id for async flows.
    """
    payload = await request.json()
    req_id = _generate_request_id()
    payload.setdefault("request_id", req_id)
    payload.setdefault("mode", payload.get("mode", "selection"))

    analysis_logger.info("Analyze entry request_id=%s mode=%s AN_MODE=%s", req_id, payload.get("mode"), ANALYSIS_MODE)

    # Dispatch by ANALYSIS_MODE
    if ANALYSIS_MODE == "simple":
        # try in-process first
        try:
            inproc = _call_simple_inprocess(payload)
            if inproc is not None:
                return {"request_id": req_id, "model_path": "simple_inproc", "result": inproc}
        except Exception as e:
            analysis_logger.exception("simple inproc failed: %s", e)

        # fallback to HTTP simple endpoint
        try:
            result = await _http_post("/api/recommend-language-simple", payload)
            return {"request_id": req_id, "model_path": "simple_http", "result": result}
        except httpx.HTTPError as e:
            analysis_logger.exception("simple http call failed: %s", e)
            raise HTTPException(status_code=502, detail="Simple analysis service unavailable")

    elif ANALYSIS_MODE == "hybrid":
        # route to hybrid controller (in-process preferred, else HTTP fallback)
        try:
            from hybrid_controller import handle_hybrid_request  # type: ignore
            result = await handle_hybrid_request(payload)
            # hybrid controller should return a dict { request_id, result } or job_id
            return {"request_id": req_id, "model_path": "hybrid_inproc", "result": result}
        except Exception as e:
            analysis_logger.warning("hybrid inproc unavailable or failed: %s", e)
            # fallback to an HTTP hybrid endpoint if you host one
            try:
                result = await _http_post("/api/hybrid/analyze", payload)
                return {"request_id": req_id, "model_path": "hybrid_http", "result": result}
            except httpx.HTTPError as e2:
                analysis_logger.exception("hybrid http call failed: %s", e2)
                raise HTTPException(status_code=502, detail="Hybrid analysis service unavailable")

    elif ANALYSIS_MODE == "legacy":
        # prefer in-process legacy
        inproc = _call_legacy_inprocess(payload)
        if inproc is not None:
            return {"request_id": req_id, "model_path": "legacy_inproc", "result": inproc}
        # fallback to known legacy endpoint path
        try:
            result = await _http_post("/api/recommend-language-legacy", payload)
            return {"request_id": req_id, "model_path": "legacy_http", "result": result}
        except httpx.HTTPError as e:
            analysis_logger.exception("legacy http call failed: %s", e)
            raise HTTPException(status_code=502, detail="Legacy analysis service unavailable")

    else:
        raise HTTPException(status_code=400, detail=f"Unsupported ANALYSIS_MODE={ANALYSIS_MODE}")
# ---- END PATCH: Analysis Mode Routing ----

@app.on_event("startup")
async def startup_event():
    """Initialize enterprise AI service on startup"""
    global enterprise_ai_service
    
    logger.info("üöÄ Starting Enterprise Ilana AI Service")
    
    if ENTERPRISE_AVAILABLE:
        try:
            # Debug environment variables
            logger.info(f"üîç Environment check - AZURE_OPENAI_ENDPOINT: {os.getenv('AZURE_OPENAI_ENDPOINT', 'NOT SET')[:50]}...")
            logger.info(f"üîç Environment check - AZURE_OPENAI_API_KEY: {'SET' if os.getenv('AZURE_OPENAI_API_KEY') else 'NOT SET'}")
            logger.info(f"üîç Environment check - AZURE_OPENAI_DEPLOYMENT: {os.getenv('AZURE_OPENAI_DEPLOYMENT', 'NOT SET')}")
            logger.info(f"üîç Environment check - ENABLE_AZURE_OPENAI: {os.getenv('ENABLE_AZURE_OPENAI', 'NOT SET')}")
            
            # Load enterprise configuration
            config = get_config("production")
            enterprise_ai_service = create_optimized_real_ai_service(config)
            logger.info("‚úÖ Enterprise AI service initialized with full stack (Azure OpenAI + Pinecone + PubMedBERT)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Enterprise AI service initialization failed: {e}")
            logger.warning(f"‚ö†Ô∏è Full error details: {str(e)}")
            enterprise_ai_service = None
    else:
        logger.warning("‚ö†Ô∏è Enterprise AI components not available, using fallback analysis")
        enterprise_ai_service = None
    
    logger.info("‚úÖ Enterprise AI production deployment ready with full stack")

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "Ilana Protocol Intelligence API",
        "version": "1.2.0",
        "status": "running",
        "deployment": "production",
        "features": [
            "Protocol Issue Detection",
            "Therapeutic Area Classification", 
            "Text Enhancement",
            "Compliance Analysis",
            "Feasibility Assessment"
        ]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.2.0",
        "deployment": "production",
        "services": {
            "api": "active",
            "cors": "enabled",
            "analysis": "active"
        }
    }

@app.get("/debug/azure-openai")
async def debug_azure_openai():
    """Debug Azure OpenAI connection"""
    debug_info = {
        "environment_check": {},
        "connection_test": {},
        "configuration": {}
    }
    
    # Check environment variables
    debug_info["environment_check"] = {
        "AZURE_OPENAI_ENDPOINT": os.getenv('AZURE_OPENAI_ENDPOINT', 'NOT SET')[:50] + '...' if os.getenv('AZURE_OPENAI_ENDPOINT') else 'NOT SET',
        "AZURE_OPENAI_API_KEY": 'SET' if os.getenv('AZURE_OPENAI_API_KEY') else 'NOT SET',
        "AZURE_OPENAI_API_KEY_LENGTH": len(os.getenv('AZURE_OPENAI_API_KEY', '')),
        "AZURE_OPENAI_DEPLOYMENT": os.getenv('AZURE_OPENAI_DEPLOYMENT', 'NOT SET'),
        "ENABLE_AZURE_OPENAI": os.getenv('ENABLE_AZURE_OPENAI', 'NOT SET')
    }
    
    # Test configuration loading
    if ENTERPRISE_AVAILABLE:
        try:
            from config_loader import get_config
            config = get_config("production")
            debug_info["configuration"] = {
                "config_loaded": True,
                "enable_azure_openai": getattr(config, 'enable_azure_openai', 'NOT SET'),
                "azure_openai_endpoint": getattr(config, 'azure_openai_endpoint', 'NOT SET')[:50] + '...' if hasattr(config, 'azure_openai_endpoint') else 'NOT SET',
                "azure_openai_deployment": getattr(config, 'azure_openai_deployment', 'NOT SET'),
                "api_key_available": bool(getattr(config, 'azure_openai_api_key', None))
            }
            
            # Test Azure OpenAI connection
            try:
                from openai import AzureOpenAI
                client = AzureOpenAI(
                    api_key=config.azure_openai_api_key,
                    api_version="2024-02-01",
                    azure_endpoint=config.azure_openai_endpoint
                )
                
                # Test models list
                models = client.models.list()
                model_count = len(list(models))
                
                debug_info["connection_test"] = {
                    "client_created": True,
                    "models_accessible": True,
                    "model_count": model_count,
                    "test_status": "SUCCESS"
                }
                
            except Exception as conn_error:
                debug_info["connection_test"] = {
                    "client_created": False,
                    "error_type": type(conn_error).__name__,
                    "error_message": str(conn_error),
                    "test_status": "FAILED"
                }
                
        except Exception as config_error:
            debug_info["configuration"] = {
                "config_loaded": False,
                "error": str(config_error)
            }
    else:
        debug_info["configuration"] = {
            "enterprise_available": False,
            "message": "Enterprise components not available"
        }
    
    return debug_info

@app.get("/debug/test-azure-ai")
async def test_azure_ai_direct():
    """Direct test of Azure OpenAI to see what it returns"""
    if not enterprise_ai_service:
        return {"error": "Enterprise AI service not available"}
    
    try:
        # Test with simple medical text
        test_text = "HER2-positive breast cancer patients will receive trastuzumab. Monitor for cardiotoxicity."
        
        from optimized_real_ai_service import TADetectionResult
        # Create mock TA detection
        ta_detection = TADetectionResult(
            therapeutic_area="oncology",
            subindication="breast_cancer", 
            phase="III",
            confidence=1.0,
            confidence_scores={"oncology": 1.0},
            detected_keywords=["her2", "breast cancer", "trastuzumab"],
            reasoning="Test data"
        )
        
        # Call the chunk analysis directly
        suggestions = await enterprise_ai_service._analyze_chunk_enterprise(
            test_text, 0, ta_detection, 
            "Test vector context from Pinecone",
            "Test PubMedBERT insights"
        )
        
        return {
            "test_text": test_text,
            "suggestions_count": len(suggestions),
            "suggestions": [
                {
                    "type": s.type,
                    "subtype": s.subtype,
                    "original": s.originalText[:100],
                    "suggested": s.suggestedText[:100],
                    "backend": s.backendConfidence
                } for s in suggestions[:3]
            ]
        }
        
    except Exception as e:
        return {"error": f"Direct Azure AI test failed: {str(e)}"}

async def simple_recommend_language(content: str, chunk_index: int = 0, total_chunks: int = 1) -> Dict[str, Any]:
    """Simple Azure OpenAI recommendation function - direct calls without complex pipeline"""
    logger.info(f"üöÄ SIMPLE AI: Direct Azure OpenAI analysis for chunk {chunk_index + 1}/{total_chunks}")
    
    try:
        from openai import AzureOpenAI
        from config_loader import get_config
        
        config = get_config("production")
        client = AzureOpenAI(
            api_key=config.azure_openai_api_key,
            api_version="2024-02-01",
            azure_endpoint=config.azure_openai_endpoint
        )
        
        # Direct medical prompt - no complex abstraction layers
        prompt = f"""You are a pharmaceutical protocol optimization AI. Analyze this text and provide specific medical recommendations.

Text to analyze:
{content}

Provide exactly 3-5 specific medical recommendations in this format:
1. Original: "patient" ‚Üí Suggested: "participant" (Reason: ICH-GCP compliance for clinical research)
2. Original: "HER2+ breast cancer" ‚Üí Suggested: "HER2+ breast cancer with trastuzumab-related cardiotoxicity monitoring per ACC/AHA guidelines" (Reason: Drug-specific safety monitoring)

Focus on:
- Medical terminology corrections (patient ‚Üí participant)
- Drug-specific monitoring requirements (trastuzumab cardiotoxicity, CDK4/6 neutropenia)
- ICH-GCP compliance improvements
- Regulatory guidance enhancements

Format each recommendation as: Original: "text" ‚Üí Suggested: "text" (Reason: explanation)"""

        response = client.chat.completions.create(
            model=config.azure_openai_deployment,
            messages=[
                {"role": "system", "content": "You are a pharmaceutical protocol AI providing specific medical recommendations."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.3
        )
        
        ai_response = response.choices[0].message.content
        logger.info(f"‚úÖ SIMPLE AI: Got Azure OpenAI response ({len(ai_response)} chars)")
        
        # Parse simple text response into suggestions
        suggestions = []
        lines = ai_response.strip().split('\n')
        
        for i, line in enumerate(lines):
            if '‚Üí' in line and 'Original:' in line and 'Suggested:' in line:
                try:
                    # Extract original and suggested text
                    parts = line.split('‚Üí')
                    original_part = parts[0].strip()
                    suggested_part = parts[1].strip()
                    
                    original_text = original_part.split('Original:')[1].strip().strip('"').strip("'")
                    suggested_full = suggested_part.split('(Reason:')[0].strip()
                    suggested_text = suggested_full.split('Suggested:')[1].strip().strip('"').strip("'")
                    
                    reason = ""
                    if '(Reason:' in suggested_part:
                        reason = suggested_part.split('(Reason:')[1].strip().rstrip(')')
                    
                    suggestions.append({
                        "id": f"simple_ai_chunk_{chunk_index}_issue_{i}",
                        "type": "medical_terminology",
                        "severity": "medium",
                        "text": original_text,
                        "suggestion": suggested_text,
                        "rationale": reason,
                        "regulatory_source": "Azure OpenAI Medical Analysis",
                        "position": {"start": i * 20, "end": i * 20 + len(original_text)},
                        "category": "medical_enhancement",
                        "confidence": 0.9,
                        "ai_enhanced": True,
                        "simple_ai_analysis": True,
                        "backend_confidence": "azure_openai_direct"
                    })
                except Exception as parse_error:
                    logger.warning(f"Failed to parse AI response line: {line} - {parse_error}")
                    continue
        
        logger.info(f"‚úÖ SIMPLE AI: Parsed {len(suggestions)} medical recommendations")
        
        return {
            "suggestions": suggestions,
            "metadata": {
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
                "content_length": len(content),
                "suggestions_generated": len(suggestions),
                "simple_ai_enabled": True,
                "processing_time": 0.8,
                "model_version": "2.0.0-simple-azure-direct",
                "ai_stack": "Azure OpenAI Direct",
                "ai_response_length": len(ai_response)
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Simple AI analysis failed: {e}")
        # Return basic fallback
        return {
            "suggestions": [{
                "id": f"fallback_chunk_{chunk_index}_issue_0",
                "type": "terminology",
                "severity": "medium", 
                "text": "patient",
                "suggestion": "participant",
                "rationale": "Use 'participant' instead of 'patient' per ICH-GCP guidelines",
                "regulatory_source": "ICH-GCP Guidelines",
                "position": {"start": 0, "end": 7},
                "category": "compliance",
                "confidence": 0.8,
                "ai_enhanced": False,
                "simple_ai_analysis": False
            }],
            "metadata": {
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
                "content_length": len(content),
                "suggestions_generated": 1,
                "simple_ai_enabled": False,
                "processing_time": 0.1,
                "model_version": "1.0.0-fallback",
                "ai_stack": "Fallback Pattern",
                "error": str(e)
            }
        }

@app.post("/api/recommend-language")
async def recommend_language_route(request: ComprehensiveAnalysisRequest):
    """Language recommendation endpoint with simple/legacy pipeline routing"""
    try:
        content = request.text
        chunk_index = getattr(request, 'chunk_index', 0) 
        total_chunks = getattr(request, 'total_chunks', 1)
        
        # Check environment flag for simple vs legacy pipeline
        use_simple_azure = os.getenv("USE_SIMPLE_AZURE_PROMPT", "true").lower() == "true"
        logger.info(f"üîÄ ROUTING: USE_SIMPLE_AZURE_PROMPT={use_simple_azure}")
        
        if use_simple_azure:
            # Route to simple Azure OpenAI function
            result = await simple_recommend_language(content, chunk_index, total_chunks)
            result["metadata"]["pipeline_used"] = "simple_azure_direct"
            return result
        else:
            # Route to legacy enterprise pipeline
            logger.info(f"ü§ñ LEGACY PIPELINE: Analyzing chunk {chunk_index + 1}/{total_chunks} ({len(content)} chars)")
            
            if enterprise_ai_service:
                try:
                    # Call legacy enterprise AI stack (Azure OpenAI + Pinecone + PubMedBERT)
                    suggestions, metadata = await enterprise_ai_service.analyze_comprehensive(
                        content,
                        {"chunk_index": chunk_index, "total_chunks": total_chunks}
                    )
                    
                    # Convert to API response format
                    issues = []
                    for suggestion in suggestions:
                        issues.append({
                            "id": f"legacy_chunk_{chunk_index}_issue_{len(issues)}",
                            "type": suggestion.type,
                            "severity": suggestion.subtype.replace("enterprise_", "") if suggestion.subtype else "medium",
                            "text": suggestion.originalText,
                            "suggestion": suggestion.suggestedText,
                            "rationale": suggestion.rationale,
                            "regulatory_source": suggestion.guidanceSource or "Legacy Enterprise AI Analysis",
                            "position": suggestion.range if suggestion.range else {"start": 0, "end": len(suggestion.originalText)},
                            "category": suggestion.type,
                            "confidence": 0.95,  # Enterprise AI confidence
                            "ai_enhanced": True,
                            "legacy_enterprise_analysis": True,
                            "backend_confidence": suggestion.backendConfidence,
                            "compliance_rationale": suggestion.complianceRationale,
                            "fda_reference": suggestion.fdaReference,
                            "ema_reference": suggestion.emaReference,
                            "operational_impact": suggestion.operationalImpact,
                            "retention_risk": suggestion.retentionRisk
                        })
                    
                    logger.info(f"‚úÖ LEGACY PIPELINE: Generated {len(issues)} pharma-grade suggestions using full AI stack")
                    
                    # Return legacy enterprise AI response
                    return {
                        "suggestions": issues,
                        "metadata": {
                            "chunk_index": chunk_index,
                            "total_chunks": total_chunks,
                            "content_length": len(content),
                            "suggestions_generated": len(issues),
                            "enterprise_ai_enabled": True,
                            "processing_time": metadata.get("processing_time", 0),
                            "model_version": metadata.get("model_version", "5.0.0-full-enterprise-stack"),
                            "ai_stack": "Legacy Azure OpenAI + Pinecone + PubMedBERT",
                            "pipeline_used": "legacy_enterprise",
                            "therapeutic_area_detection": metadata.get("therapeutic_area_detection", {}),
                            "enterprise_features": metadata.get("enterprise_features", {})
                        }
                    }
                    
                except Exception as ai_error:
                    logger.error(f"‚ùå Legacy Enterprise AI analysis failed: {ai_error}")
                    # Fall through to pattern-based analysis
            
            # Legacy fallback pattern-based analysis
            logger.warning("‚ö†Ô∏è Using legacy fallback pattern analysis")
            issues = []
            sentences = content.split('.')
            
            # Enhanced fallback patterns with specific replacements
            fallback_patterns = [
                ("patient", "participant", "compliance", "Use 'participant' instead of 'patient' per ICH-GCP guidelines for clinical research"),
                ("patients", "participants", "compliance", "Use 'participants' instead of 'patients' per ICH-GCP guidelines for clinical research"),
                ("will be", "shall be", "compliance", "Use 'shall be' instead of 'will be' for protocol requirements per regulatory standards"),
                ("daily", "once daily", "feasibility", "Consider specifying 'once daily' for clarity and reducing participant burden"),
                ("every day", "once daily", "clarity", "Use standardized terminology 'once daily' instead of 'every day'"),
                ("twice a day", "twice daily", "clarity", "Use standardized terminology 'twice daily' instead of 'twice a day'"),
                ("morning", "in the morning", "clarity", "Specify 'in the morning' for clearer dosing instructions"),
                ("evening", "in the evening", "clarity", "Specify 'in the evening' for clearer dosing instructions"),
                ("doctor", "investigator", "compliance", "Use 'investigator' instead of 'doctor' per clinical research standards"),
                ("study drug", "investigational product", "compliance", "Use 'investigational product' instead of 'study drug' per ICH-GCP terminology"),
                ("side effect", "adverse event", "compliance", "Use 'adverse event' instead of 'side effect' per ICH-GCP guidelines"),
                ("side effects", "adverse events", "compliance", "Use 'adverse events' instead of 'side effects' per ICH-GCP guidelines")
            ]
            
            for i, sentence in enumerate(sentences[:10]):
                sentence = sentence.strip()
                if len(sentence) < 10:
                    continue
                
                for pattern, replacement, issue_type, rationale in fallback_patterns:
                    if pattern in sentence.lower():
                        # Find the exact text and create replacement
                        original_text = sentence
                        suggested_text = sentence.replace(pattern, replacement)
                        
                        # If the replacement is the same as original, skip
                        if original_text == suggested_text:
                            continue
                        
                        issues.append({
                            "id": f"legacy_fallback_chunk_{chunk_index}_issue_{len(issues)}",
                            "type": issue_type,
                            "severity": "medium",
                            "text": original_text[:150] + "..." if len(original_text) > 150 else original_text,
                            "suggestion": suggested_text[:200] + "..." if len(suggested_text) > 200 else suggested_text,
                            "rationale": rationale,
                            "regulatory_source": "ICH-GCP Guidelines",
                            "position": {"start": i * 50, "end": i * 50 + len(sentence)},
                            "category": issue_type,
                            "confidence": 0.8,
                            "ai_enhanced": False,
                            "legacy_enterprise_analysis": False,
                            "original_term": pattern,
                            "suggested_term": replacement
                        })
                        break
            
            return {
                "suggestions": issues,
                "metadata": {
                    "chunk_index": chunk_index,
                    "total_chunks": total_chunks,
                    "content_length": len(content),
                    "suggestions_generated": len(issues),
                    "enterprise_ai_enabled": False,
                    "processing_time": 0.1,
                    "model_version": "1.0.0-legacy-pattern-fallback",
                    "ai_stack": "Legacy Pattern matching fallback",
                    "pipeline_used": "legacy_fallback"
                }
            }
        
    except Exception as e:
        logger.error(f"‚ùå Language recommendation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Language recommendation failed: {str(e)}")

@app.post("/analyze-comprehensive")
async def analyze_comprehensive(request: ComprehensiveAnalysisRequest):
    """Legacy comprehensive analysis endpoint - preserved for backward compatibility"""
    return await recommend_language_route(request)

@app.post("/api/ta-detect")
async def detect_therapeutic_area_simple(request: TADetectRequest):
    """Simplified therapeutic area detection endpoint"""
    try:
        logger.info(f"üéØ TA detection for {len(request.content)} characters")
        
        content_lower = request.content.lower()
        
        ta_keywords = {
            "cardiovascular": ["heart", "cardiac", "cardiovascular", "blood pressure", "hypertension"],
            "oncology": ["cancer", "tumor", "oncology", "chemotherapy", "radiation"],
            "endocrinology": ["diabetes", "insulin", "glucose", "thyroid", "hormone"],
            "neurology": ["brain", "neurological", "seizure", "stroke", "dementia"]
        }
        
        detected_ta = "general_medicine"
        confidence = 0.5
        keywords_found = []
        
        for ta, keywords in ta_keywords.items():
            matches = [kw for kw in keywords if kw in content_lower]
            if matches:
                detected_ta = ta
                confidence = min(1.0, 0.8 + len(matches) * 0.1)
                keywords_found = [f"{kw} (1x)" for kw in matches[:5]]
                break
        
        return {
            "therapeutic_area": detected_ta,
            "confidence": confidence,
            "keywords_found": keywords_found,
            "alternative_areas": [
                {"area": "general_medicine", "confidence": 0.3} if detected_ta != "general_medicine" else {"area": "oncology", "confidence": 0.2}
            ]
        }
        
    except Exception as e:
        logger.error(f"‚ùå TA detection failed: {e}")
        raise HTTPException(status_code=500, detail=f"TA detection failed: {str(e)}")

@app.post("/api/enhance-text")
async def enhance_text_simple(request: TextEnhanceRequest):
    """Simplified text enhancement endpoint"""
    try:
        logger.info(f"ü§ñ Text enhancement for {len(request.original_text)} chars")
        
        enhanced_text = request.original_text
        explanation = f"Enhanced for {request.therapeutic_area} protocol compliance"
        
        if "patient" in request.original_text.lower():
            enhanced_text = enhanced_text.replace("patient", "subject").replace("Patient", "Subject")
            explanation += ". Changed 'patient' to 'subject' per ICH-GCP guidelines."
        
        return {
            "enhanced_text": enhanced_text,
            "explanation": explanation,
            "confidence": 0.85,
            "regulatory_basis": [
                {
                    "source": "ICH E6(R2) GCP Guidance",
                    "relevance": 0.85,
                    "citation": "Good Clinical Practice guidelines"
                }
            ],
            "therapeutic_area": request.therapeutic_area,
            "enhancement_type": request.enhancement_type,
            "improvements": ["Terminology standardization"]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Text enhancement failed: {e}")
        raise HTTPException(status_code=500, detail=f"Text enhancement failed: {str(e)}")

@app.post("/api/ta-recommendations")
async def get_ta_recommendations_simple(request: TARecommendationsRequest):
    """Simplified TA recommendations endpoint"""
    try:
        logger.info(f"üè• TA recommendations for {request.therapeutic_area}")
        
        recommendations = [
            {
                "title": "ICH-GCP Compliance",
                "content": "Ensure all trial procedures follow ICH Good Clinical Practice guidelines",
                "priority": "high",
                "regulatory_source": "ICH E6(R2) GCP Guidance"
            },
            {
                "title": "Statistical Analysis Plan", 
                "content": "Develop comprehensive SAP addressing endpoints and multiplicity",
                "priority": "high",
                "regulatory_source": "ICH E9 Statistical Principles"
            }
        ]
        
        return {
            "therapeutic_area": request.therapeutic_area,
            "protocol_type": request.protocol_type,
            "recommendations": recommendations,
            "total_recommendations": len(recommendations),
            "last_updated": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå TA recommendations failed: {e}")
        raise HTTPException(status_code=500, detail=f"TA recommendations failed: {str(e)}")

# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Handle HTTP exceptions"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Handle general exceptions"""
    logger.error(f"Unhandled exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "timestamp": datetime.utcnow().isoformat(),
            "path": str(request.url)
        }
    )

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        log_level="info"
    )