Pre-Analysis Plan
Minnesota COVID-19 Testing
This Draft: September 24, [ADDRESS_939485]
underserved communities and don’t address fears of social stigma, mistrust in the healthcare system,
or concerns about immigration status.
The goal of this project is to help the state of Minnesota understand why individuals are not getting
tested and potentially identify trusted individuals or organizations that could be used in follow-up
work to send messages. To do so, we are deploying ﬂyers through [ADDRESS_939486] to academicians and policymakers alike.
According to Meyer, Mok, and Sullivan (2015) the quality of household surveys is in decline, for three
main reasons. First, households have become increasingly less likely to answer surveys at all (unit non-
response). Second, those that respond are less likely to answer certain questions (item nonresponse).
Third, when households do provide answers, they are less likely to be accurate (measurement error).
This is important since household surveys help to estimate the employment rate, healthcare needs and
of course the census determines resources/representation.
We focus on the ﬁrst two issues of unit and item nonresponse, which is not random across the popula-
tion and thus could lead to nonresponse bias .1Grifﬁn (2002) found that census tracts with predominantly
Hispanic or Black residents had signiﬁcantly lower response rates to the American Community Survey
as compared to the response rates in predominantly white tracts. Similarly, Maitland et al. (2017) found
that response rates to the Health Information National Trends Survey (HINTS) were lower in areas with
higher levels of Hispanic and minority residents.2
We hypothesize that ﬁnancial incentives may encourage unit response; conversely, a close association
with the government may discourage response. To test these hypotheses, we plan to cross-randomize
the incentive amount offered and the emphasis placed on government involvement in the study on ﬂy-
ers advertising our survey. Individuals will see either a) a 10 dollar incentive, or b) a 20 dollar incentive;
and either a) messaging that emphasizes government involvement in the study, or b) messaging that
1Nonresponse bias is often deﬁned as the product of the difference in mean responses between those who answer vs.
those that do not, and the nonresponse rate, see Maitland et al. (2017).
2see also https://www.nytimes.com/interactive/2015/04/20/upshot/missing-black-men.html
[ADDRESS_939487] what affects item non-response on potentially sensitive questions, such as questions which ask
for health information, we hypothesize that ethical framing may encourage individuals to answer ques-
tions. This takes two forms — the deontological (or duty based) frame, and the consequential (or cost-
beneﬁt) frame. Moreover, knowing others feel the same way (regarding the obligation or beneﬁts of
providing health information) may amplify motivation. Finally, there is the possibility that emphasiz-
ing the importance of ethnic and racial disadvantage associated with COVID-19 outcomes may also be
important.
Upon completion of the demographic module of the survey but prior to starting several potentially
sensitive survey modules, individuals will see a message that either a) emphasizes the public health
beneﬁts of answering the survey questions (cost-beneﬁt frame); b) emphasizes an individual’s respon-
sibility to their community (duty frame); c) emphasizes the disproportionate impact of COVID-19 on
certain ethnic and racial groups; or d) provides no messaging. Messaging content will be randomized
at the individual level.
2. Sampling and Experimental Protocols
2.[ADDRESS_939488]
ﬂyers (see Figure 1 and 2) advertising the baseline survey in bags of food and prepackaged meals that
are distributed to food shelf users. We require all participants to be age 18 or older and to speak English
or Spanish. We will use a touchless delivery systems to drop off and redeem ﬂyers on a daily basis. Our
target sample size is 1000 survey responses. We may stay in the ﬁeld (i.e. oversample) at foodshelves
that tend to serve minority individuals so that we can increase their representation in the survey.
2.2 Experimental Protocols
The structure of our experiment is as follows. Figure 3 provides additional information on study ﬂow.
1. Randomization of Flyers (Unit Response)
(a) Randomly assign survey advertising ﬂyers to food shelf-dates. The unit of analysis will be
the food shelf-date as will the level of randomization. See Appendix A for ﬂyer examples.
(b) Food shelves distribute the randomized daily ﬂyer via food bags and prepackaged meals.
2. Recruitment and Baseline Survey
(a) For all potential study participants, elicit a preference for English or Spanish. Those with a
preference for Spanish will be given a Spanish language consent form and survey questions.
(b) Individuals provide consent.
(c) Collect demographic information.
2
3. Randomization of messaging content before sensitive survey modules (Item Response)
(a) After completing the demographics survey module, participants are randomly assigned to
one of four ethical motivation messaging options; deontological, consequential, acknowl-
edgement of racial inequities, or nothing.
(b) Participants are shown the same randomly assigned message prior to each sensitive survey
module.
(c) Participants answer questions in survey modules 2-[ADDRESS_939489] errors will be used as the level of outcome is the same as the unit of random-
ization. For item nonresponse outcomes, we will randomize at the individual level in Qualtrics.
2.4 Survey Messaging Content
One of the following message options will be randomly assigned to participants after completing the
demographics section of the survey module.
1.No framing – No message displayed.
2.Consequential framing – "Please answer the questions in this survey. It is an easy way that you
can help improve the public health response to COVID-19 in your community."
3.Deonteological framing – "Please answer the questions in this survey. It is important for everyone
to do their part to protect their community during the COVID-19 pandemic."
4.Acknowledgement of racial inequities – "Please answer the questions in this survey. COVID-[ADDRESS_939490]."
3. Hypotheses Tested
In this study, we seek to understand survey takeup and messaging surrounding COVID-19 in minority
and low SES populations.
H1. Do higher monetary incentives increase unit response?
3
H2. Does a government frame reduce unit response?
H3. Are incentives and a de-emphasis on government complements or substitutes in increasing unit
response?
H4. a. Which frame/incentive combinations diffuse most from the initial point of distribution through-
out the community?
b. How do characteristics of individuals nudged that respond differ by [CONTACT_3148] (i.e. frames
and incentive structures)?
c. Can we extrapolate using statistical methods and the randomized estimates to obtain popu-
lation level estimates and approximate missing mass in government surveys?
H5. Do various ethical and (racial/ethnic) acknowledgement frames improve item nonresponse? (in-
cluding quality)
H6. How do item nonresponse frames interact with variation in the composition of respondents in-
duced through the randomized incentives/frames?
4. Data Collection and Outcomes
We will run our experiment beginning on September 28, 2020.
Flyers advertising the baseline survey will be distributed to the 10 participating food shelves for two
weeks. All of the survey responses will be downloaded in a .csv ﬁle for cleaning and analysis in Stata.
4.1 Oversampling by [CONTACT_692687], we
may oversample from food shelves that are located in areas with higher concentrations of these popu-
lations. In particular, we may extend the enrollment period in these speciﬁc food shelves.
4.2 Baseline
The baseline survey includes demographic characteristics, attitudes towards different media sources,
and healthcare experience.
• Gender and age
• Current residence and country of origin
• Education
• Income
• Insurance
• Media attitudes
• COVID-19 knowledge
4
• Healthcare usage and attitudes on the COVID-19 pandemic
• Discrimination in healthcare
4.3 Data quality checks
Within the survey, we will include a question aimed at capturing respondent attention. We will indicate
whether respondents are among the top 5% fastest in terms of total time spent on the survey questions.
We will deﬁne different samples based on quality cutoffs as well as assess whether quality of responses
is affected by [CONTACT_692688]. We will collect phone numbers in order to distribute electronic gift cards.
Phone numbers will also be used to identify and drop duplicate survey responses.
4.[ADDRESS_939491] been given the randomized messaging content but then
dropout of the survey and do not complete it within four days of initiation. Attrition will be assessed
in real time in order to assess any problematic survey questions. We will check that attrition is not
differential across study arms.
5. Empi[INVESTIGATOR_213459]
5.1 Econometric Speciﬁcation
Our primary speciﬁcation for evaluating H1,H2, and H3(unit nonresponse) is as follows:
Yjt=a+b1 1high _incentive
jt+b2 1research
jt +b3 1researchhigh _incentive
jt+X0
jW+FSDate +ejt (1)
Where Yis a measure of unit response at foodshelf jon date t. Since we have no characteristics on
individuals who did not visit the landing page, we run this speciﬁcation at that foodshelf-date level.
1researchis an indicator for the framing used on the advertising ﬂyers where a value equal to 0 indicates
government sponsor framing and a value equal to 1 indicates academic research sponsor framing.
1high _Incentiveis an indicator for the $20 incentive amount – the $10 category will be omitted.
We will include food shelf-date ﬁxed effects. We may also include a set of background characteristics
on the foodshelf Xin speciﬁcations chosen by [CONTACT_692689] (Chernozhukov et al., 2016). Inclusion of these
LASSO covariates may increase the precision of our treatment effect estimates.3
The primary outcome Ymeasure for evaluating unit nonresponse in Equation 1 (hypotheses H1-H3)
will be visiting the survey link provided on the advertising ﬂyer. This will be calculated as the fraction
of distributed ﬂyers at a given foodshelf that are redeemed. The denominator will come from the
3Other ﬁxed effects might also be important for precision, e.g. day of week and food shelf.
[ADDRESS_939492] provided the given the passcode on the
ﬂyer (which is uniquely associated with a given foodshelf-date) and respond that they learned about
the survey through the foodshelf (an early question on the survey).
We will also explore a secondary outcome of completion of the entire survey calculated using the same
denominator and numerator, conditional on completion.
1.H1, providing higher incentives, predicts that b1>0.
2.H2, providing research framing, predicts that b2>0.
3.H3, the interaction effect between incentives and researcher framing, is indifferent to the sign on
b3.b3>0 suggests de-emphasis of government involvement and higher incentives are comple-
mentary. b3<0 suggests they are substitutes.
4.H4a replaces the outcome with the difference between all landing page visits from respondents
with a given foodshelf-date with all those who were recruited from the foodshelf. The difference
reﬂects the ampliﬁcation of the ﬂyers throughout social networks. The denominator will continue
to be the number of ﬂyers distributed at that foodshelf-date.
Yijt=a+b1 1high _incentive
jt+b2 1research
jt +b3 1researchhigh _incentive
jt+FSDate +ejt (2)
To evaluate H4b the outcome of Equation [ADDRESS_939493] responded. For those who respond we will compare the characteristics (e.g. income, ethnic-
ity/race, age, gender) of those that respond to a low vs. high incentive and emphasis vs. de-emphasis
government.4.
See Section 5.[ADDRESS_939494] of ethical messaging on item nonresponse will be assessed according to
the following speciﬁcation:
Yi=a+l1 1community
i+l2 1duty
i+l3 1burden
i +b1 1research
jt +b2 1high _incentive
jt+X0
ijtW+FSDate +eijt(3)
Where Yiis a measure of responsiveness to sensitive questions conditional on takeup of the survey.
We are precoding the sensitivity of each question asked in a "sensitive block" by [CONTACT_692690] a measure for sensitivity. The outcome will therefore be a) a binary outcome
that indicates whether the respondent answered any sensitive questions, the b) fraction of sensitive
questions answered, c) the mean sensitivity across sensitive questions answered, and the d) overall
sum. Note - we will code answered as 1 if answer the question and do not chose "prefer not to answer".
Symmetrically, non-response will be coded as skippi[INVESTIGATOR_007] a question or choosing "prefer not to answer"
and total and fraction non-response will be an additional outcome. community is an indicator equal to
4Note - we may also receive administrative data on some group characteristics allowing us to run a version like Equation
[ADDRESS_939495] and beneﬁts of answering the survey question is displayed. duty
is an indicator equal to 1 if the message appealing to a participant’s sense of of community is displayed.
burden is an indicator equal to [ADDRESS_939496] a speciﬁcation for item nonresponse that
includes the full set of interaction terms among the incentive, framing, and messaging treatments. We
hypothesize the l1>0,l2>0 and l3>0. It will also be of interest to test for differences across the
treatments.
5.[ADDRESS_939497] a series of balance tests across treatment arms to ensure that there are no chance dif-
ferences between subjects in the various arms. For the unit nonresponse, we will look at differences in
characteristics of food shelves across treatment groups including food shelf-date ﬁxed effects.
For the item nonresponse, balance tests will be conducted at the individual level on the following
baseline characteristics.
• Survey language selected
• Sex
• Age
• Household size
• Race
• Education
• Income
• Insurance coverage
5.3 Heterogeneous Effects
Important secondary analyses will include investigating variation in the treatment response. In partic-
ular, we are interested in heterogeneity by:
• Age
• Sex
• Education
• Race
• Food pantry locations
We may also pursue a machine learning-driven approach to uncover heterogeneous treatment effects
using the techniques from Chernozhukov et al. (2017) and Chernozhukov et al. (2018b).
[ADDRESS_939498] of non-government framing on unit response compute minimum
detectable effects (MDEs) conditional on given levels of control group unit response (i.e. take-up),
which we vary from 10% to 30%. We also vary the amount of intra-cluster correlation (ICC) as our unit
of randomization is the foodshelf-date. Our calculations assume that our data features 74 food shelf-
dates, evenly allocated to two framing arms — government and researcher — with an average of 100
ﬂyers distributed per food shelf-date.
Fixing government framing as the control group and control take-up at 10%, our calculations imply a
MDE of 2.07 percentage points (or 20.7% greater than the control group mean) in the absence of intra-
cluster correlation. Holding control take-up constant at 10%, the required MDE rises to 3.68 percentage
points (+36.8%) given 2% intra-cluster correlation and to 7.46 percentage points (+74.6%) given 10%
ICC. If take-up in the control group is 30%, the required MDE falls to 3.07 percentage points (or 10.2%
greater than the control mean) in the absence of ICC, 5.34 percentage points (+17.8%) given 2% ICC,
and 10.35 percentage points (+34.5%) given 10% ICC.
In computing MDEs for item response, we assume our target sample size of 1000 survey respondents
is evenly divided across four messaging arms. Given 70% item response in the control group of no
messaging, we would require a 10.[ADDRESS_939499] size (or 15.38% higher than the control
mean) for treatment effects to be detectable. As item response in the control group rises, MDEs fall:
given 90% item response in the no messaging group, the MDE is 6.31 percentage points (+7.01% from
the control mean).
7. Funding and Human Subjects Review
Funding is provided by [CONTACT_941] J-PAL State and Local Innovation Initiative. Study approval has been
granted by [CONTACT_692691].
8. References
[1] Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, and
Whitney Newey. “Double/Debiased/Neyman Machine Learning of Treatment Effects.” American
Economic Review 107, no. 5 (2017): 261-65. https://10.1257/aer.p20171038.
8
[2] Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen, Whitney
Newey, and James Robins. “Double/Debiased Machine Learning for Treatment and Structural Pa-
rameters.” The Econometrics Journal 21, no. 1 (2018a): C1–68. https://doi.org/10.1111/ectj.[ZIP_CODE].
[3] Chernozhukov, Victor, Mert Demirer, Esther Duﬂo, and Iván Fernández-Val. “Generic Ma-
chine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments.”
Working Paper. Working Paper Series. National Bureau of Economic Research, June 2018b.
https://doi.org/10.3386/w24678.
[4] Grifﬁn, Deborah H. “Measuring Survey Nonresponse by [CONTACT_216043].” [ADDRESS_939500], Washington, DC [ZIP_CODE] Bureau of the Census: [LOCATION_002] Bureau of the Census,
2002.
[5] Maitland, Aaron, Amy Lin, David Cantor, Mike Jones, Richard P . Moser, Bradford W. Hesse,
Terisa Davis, and Kelly D. Blake. “A Nonresponse Bias Analysis of the Health Information Na-
tional Trends Survey (HINTS).” Journal of Health Communication 22, no. 7 (July 2017): 545–53.
https://doi.org/10.1080/10810730.2017.1324539.
[6] Meyer, Bruce D., Wallace K. C. Mok, and James X. Sullivan. 2015. “Household Surveys in Crisis.”
Journal of Economic Perspectives, 29 (4): 199-226.
[7] Wolfers, Justin, David Leonhardt, and Kevin Quealy. “1.5 Million Missing Black Men.” The New
York Times, April 20, 2015, sec. The Upshot.
9
Appendix
A. Survey Advertising Fliers
Figure 1: Government Framing Flier
10
Figure 2: Academic Researcher Framing Flier
11
Figure 3: Consort Diagram
B. Power Calculations
All power calculations are shown in Table 1 and Table 2 are MDEs assuming aof 0.05 and power of
80%. MDEs are displayed both as percentage points and as percentage of the control group mean.
Table 1: Unit Response MDEs by [CONTACT_692692]-Cluster Correlation
Control Group Response r MDE (Percentage Points) MDE
10% 0% 2.07 20.7%
10% 2% 3.68 36.8%
10% 4% 4.84 48.4%
10% 6% 5.81 58.1%
10% 8% 6.67 66.7%
10% 10% 7.46 74.6%
20% 0% 2.70 13.5%
20% 2% 4.74 23.7%
20% 4% 6.18 30.9%
20% 6% 7.37 36.8%
20% 8% 8.41 42.0%
20% 10% 9.34 46.7%
30% 0% 3.07 10.2%
30% 2% 5.34 17.8%
30% 4% 6.92 23.1%
30% 6% 8.22 27.4%
30% 8% 9.34 31.1%
30% 10% 10.35 34.5%
Power calculations for unit response were conducted using the cluster-
sampsi package in STATA 16.
Table 2: Item Response MDEs by [CONTACT_692693] (Percentage Points) MDE
70% 10.77 15.38%
75% 10.00 13.33%
80% 9.04 11.30%
85% 7.85 9.23%
90% 6.31 7.01%
13