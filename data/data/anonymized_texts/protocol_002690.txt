 
 
Study Title: Evaluation of a Crowd- Powered Web Platform for Depression and Anxiety  
 
Study ID: [REMOVED] 
 Document Date:  January 14, 2021   
 
 
Objectives :  
 
This pilot effectiveness trial aims to determine the impact of the Crowd- Powered Platform on 
symptoms of depression and anxiety and evaluate whether the platform engages the putative targets 
of personal relevance and interpersonal relationship building, resulting in increased skills mastery and skills use. We will enroll 100 participants in a two- armed randomized controlled trial of the platform. 
Participants will be randomly assigned to receive either the crowd- powered platform (treatment) or a 
similar self -guided platform (control) that contains the didactic material (learning) but none of the 
crowd features. Participants will be recruited from Mental Health America (MHA)'s Screening- to-
Supports platform, an online screening platform that conducts approximately 3000 screens each day. The MHA Screening- to-Supports platform includes screeners for depression (PHQ -9) and anxiety 
(GAD -7). Individuals who screen positive for depression or anxiety will receive brief online study 
advertisements. Study advertisements will be 1- 2 sentences long, and include the following text: 
“Click here to take part in a 16 week paid trial and test a new online platform for depression and 
anxiety”. Individuals who receive and click on these advertisements will have access to a REDCap form with screening questions regarding demographics, contact information, age, diagnostic status and English proficiency. Those who are eligible as determined by the web screener will be contacted 
by research personnel to schedule a 1 hour remote interview where they will 1) review the consent form with research staff and electronically sign via REDCap’s e- consent form 2) complete a 
diagnostic assessment with modules of the SCID. Afterward, participants will be sent a REDCap link via email to complete baseline assessments and will receive login information for their assigned platform (e.g., treatment or control) via email. Participants will use their assigned platform for 8 weeks and will be instructed to use the platform at least 3 times per week. Participants will be sent weekly newsletters via Mailchimp.com, and text message reminders to complete surveys. These newsletters will include didactic information about the platform and platform content, reminders to engage with the platform, and reminders to complete study measures. Participants will receive trial assessments at baseline, week 4, and week 8 (post -treatment). Follow -up evaluations will occur at 16 weeks to 
evaluate maintenance of gains. In addition, a subset of participants will be asked to complete a one-hour evaluation of user experience at 16 weeks.
 
 
Design:  
100 participants will be enrolled in a two- armed randomized controlled trial of the ADAPT platform. 
Participants will be randomly assigned to receive either the ADAPT platform (treatment) or a similar 
self-guided platform (control) that contains the didactic material (learning) but none of the crowd 
features (requesting or responding). Participants will use their assigned platform for a treatment period of 8 weeks. Participants will receive trial assessments at baseline, week 4, and week 8 (post -
treatment). Follow -up evaluations will occur at 16 weeks to evaluate maintenance of gains.  
 Methods : 
 Participants will be asked to interact with their assigned platform for 8 weeks, at least 3 times per week. Each platform interaction is estimated to take between 5- 10 minutes, but participants may 
choose to engage on the platform for as long as they would like. Participants will be randomized into one of two conditions, where they may either interact with a version of this platform that  includes 
content intended to teach you about mental health and skills, or a similar version that also includes peer support. Participants will be asked to complete an initial, one- time mental health assessment via 
Zoom call that will take approximately one hour. Participants will be sent weekly newsletters via Mailchimp.com. These newsletters will include didactic information about the platform and platform content, reminders to engage with the platform, and reminders to complete study measures. Email addresses will be used to send participants newsletters through Mailchimp.com. No additional 
identifying information will be shared with Mailchimp.com. Subjects may unsubscribe from Mailchimp 
 
 
newsletters at any time. When the study is complete, the contact list of subject emails will be deleted 
from Mailchimp. Participants will also complete a series of online surveys that will ask questions about their mental health, which will begin at the time of enrollment, and will be administered at 4 weeks, 8 weeks, and 16 weeks. Participants will receive text message reminders to complete surveys when they are received. Text message reminders will say the following: “Hello, this is a reminder from the Overcoming Thoughts Study to complete your [week 4, 8, 16] surveys. You must complete all surveys in order to receive the maximum payment. (These are very important to our research and we appreciate your time.) If you have any troubles, please contact us. Thank you!” At 16 weeks, an optional follow up remote interview will b e conducted with a subset of participants to evaluate 
acceptability and usability. Researchers will conduct these follow -up interviews with a subset of 
participants using purposive sampling to identify those who did or not benefit balancing across the treatment and control arm, as well as those who had low and high usage. Benefit will be defined as 
those who received at least a 25% improvement from pre- treatment score on the DASS and a shift in 
category (e.g., moderate to mild). Interviewers will be blind to treatment condition as well as benefit and usage status. Each self -report assessment and remote interview will take approximately one 
hour. The total time required for each participant (including time spent on platform, 4 self -report 
assessments, and bot h possible remote interviews) is 10 hours.  All remote interviews will be audio 
recorded for analysis and quality assurance. Researchers will gather usage data directly through the platform, such as number of log- ins and length of use per session.  
“Crowdu ser” information refers to all user -generated responses submitted on the platform, which are 
subsequently visible to all other users accessing the site. The crowdsourced platform is publicly available on Mental Health America’s website, therefore participants in the crowdsource condition may see content submitted by non- study participants. Information submitted by public users (non-
participants) will not be gathered and included in the data report that UCI researchers receive. Only data gathered from the  consented participants will be gathered for study purposes.  
 
Feasibility metrics  will also be drawn from data collected through the ADAPT platform including the 
amount of time required to produce an acceptable response to a request,  amount of time between 
notifications being sent to a user and their login to the platform. If professionals are required to generate some content, we will also estimate the cost of producing a response by tracking professional time and multiplying time by  hourly pay rate. Adherence will be tracked as the number of 
days between a user’s first and last login to the ADAPT platform as well as the number of unique days that the user visited the ADAPT platform. Users will be instructed to use the platform at least 3 times a week and we will also look at the number of weeks that users meet this expected level of use (0-8).  
Acceptability will be assessed through quantitative ratings of the ADAPT platform at end of 
treatment. Additionally, we will conduct individual interviews with a subset of participants using purposive sampling to identify those who did or not benefit balancing across the treatment and control arm. Benefit will be defined as those who received at least a 25% improvement from pre- treatment 
score o n the DASS and a shift in category (e.g., moderate to mild). Interviewers will be blind to 
treatment condition as well as benefit status.  
Target Outcomes include measures of clinical symptoms, accountability, skill use and mastery, 
functioning, and use of  the platform. We will use identical measures to the field trial. An online 
administered self -report assessment will measure depressive and anxiety symptoms using the DASS, 
accountability using the SAQ, skills use and mastery using the F requency of Actions and Thoughts  
and Coping Self -Efficacy, and functioning using the PROMIS Ability to Participate in Social Roles and 
Activities Scale at baseline, week 4, week 8, and week 16. A telephone interview will consist of the SCID- 5 for depression status and anxiety status at baseline. Use metrics include nu mber of logins, 
number of times each task is completed (learning, requesting, responding), and time spent on the platform  
 
 
 
Scientific Background : 
 
Depression and anxiety are the most common mental health disorders in the United States with 
estimated 12- month prevalence rates of 18.1% and 9.5% respectively. These disorders commonly 
co-occur and it is estimated that over 70 million Americans each year will meet diagnostic criteria for 
either depression or anxiety. The human and economic burden of depression and anxiety are monumental. Depression alone is the leading cause of disability and represents the highest disease burden worldwide. Depression and anxiety have comparable impacts on quality of life and are associated with high levels of lost work productivity and increased medical costs. Efficacious treatments exist, yet many people fail to receive them. Estimates of service utilization indicate that among those meeting criteria for a mental disorder nearly 80% receive no treatment whatsoever. Barriers to treatment include costs, time, access, and stigma associated with mental health treatment. In response to these barriers, many researchers and healthcare systems have turned to technology -
based treatment options, such as Internet websites and mobile apps, as alternatives to traditional face- to-face treatments. These interventions are referred to collectively as “behavioral intervention 
technologies” (or BITs) to distinguish them from other information and communication technologies that support healthcare more generally such as electronic medical records, clinical decision support programs, and telemedicine. As an alternative to traditional treatments, BITs allow users to access resources from anywhere in the world, at a time of their choosing. Various BITs for depression and anxiety have been evaluated with evidence supporting their efficacy. This includes Internet websites and mobile applications that draw from established evidence- based treatments such as cognitive-
behavioral therapy (CBT), or transdiagnostic approaches. Despite their efficacy, one of the primary challenges that BITs face is sustained engagement. Open access BITs have particularly low rates of adherence with several studies finding only 1% of users complete all modules. Engagement tends to be better in clinical trials of BITs, with a systematic review finding completion rates ranged from 12-100% with a median of 56%. One  characteristic that differentiated those trials with higher completion 
rates from those with lower completion rates was the use of human supported BITs. Completion rates in human supported BITs were found to be 80%, which compares favorably to face- to-face treatment. 
We think human support matters because it increases personal relevance of the techniques contained within the BIT and increases accountability. In traditional face- to-face CBT, the techniques 
factor refers to a therapist’s fidelity to evidence- based practices while also acknowledging that 
adaptations must be made for each individual. In BIT treatments, fidelity to evidence- based principles 
can be very high, as it is pre- programmed. Most BITs, however, are not adapted for an individual and 
their needs. As such, we suggest that increasing the personal relevance within BIT treatments may 
increase skills use, mastery, and ultimately functioning. Personal relevance can be increased by including human supporters to clarify the use of the BIT and provide supplemental information to improve fit and use in a person’s daily life. Human support also increases relational factors. In traditional face- to-face CBT, relational factors refer to the general qualities of the interaction between 
patient and therapis t, and the therapist’s ability to be attuned to their patients. In BIT treatments, 
relational factors have been most often described through the model of supportive accountability. In short, personally relevant therapeutic techniques and accountability dri ve skills use and mastery. 
Increased skills use and mastery results in improved functioning and clinical outcomes. Indeed, a robust literature links CBT skills use and mastery to outcome in therapy. Less research has evaluated what factors increase skills use, but BIT treatments provide a unique opportunity to explore this in detail through their ability to systematically collect data regarding skills use. This conceptual model helps illustrate why BITs that include human support have higher rates of engagement and greater efficacy. In order for BITs to reach their potential of offering scalable accessible interventions, novel strategies are needed to leverage non- professionals to support treatment engagement. This project 
explores enhancing BITs through crowd -powered processes. Crowdsourcing has been used in 
several successful technologies (e.g., reCAPTCHA, Foldit). At its core, crowdsourcing is a method for rapidly mobilizing large numbers of people to accomplish tasks on a larger scale. For effective 
 
 
crowd work, tasks need to be decomposed and distributed such that each person’s contribution 
advances the overall goal of the task. This is possible through structuring intervention content into 
discrete steps that are consistent with evidence- based treatment strategies. In this project we will 
evaluate the effectiveness of a crowd- powered treatment for depression and anxiety. The platform will 
contain structured modules that provide scaffolded instructions for evidence- based treatment 
strategies (i.e., behavioral experiments and cognitive restructuring) and will support these processes through crowd- interactions. Users of the platform can perform 3 distinct tasks: (1) learning, (2) 
requesting, and (3) responding. Learning involves reading didactic content, reques ting involves 
completing interactive forms based on the selected treatment strategies, and responding involves interacting with requests submitted by other users. Each user can perform all of these tasks, however, they will be made available on the platfor m sequentially after each user successfully 
completes prior tasks. Therefore, users are not assigned to roles from the start but rather will learn, request, and respond at different times. It has been noted that individual psychotherapy as the dominant model of treatment delivery is unlikely to meet the need for mental health services. Instead, we need novel delivery models leveraging new opportunities like technology. The future of BITs for mental health need to include interventions that are as engaging as Facebook and Twitter but as efficacious as the best evidence- based practices developed through years of psychological research. 
The aim of the current platform is to achieve this goal through combining evidence- based treatment 
strategies and peer interactions to create a system capable of refining these evidence- based 
strategies and personalizing mental health interventions.
 
 