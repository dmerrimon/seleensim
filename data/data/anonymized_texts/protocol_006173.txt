Place -Based Mapping in EAS Listeners  
 
[STUDY_ID_REMOVED]  
 
Modified Date: 2/17/2025  
  
Statistical Power and Design  
 
For the first experiment under Aim 1, we plan to investigate monaural speech for EAS users listening with a 
place -based versus a default map. We plan to recruit 40 subject s from the UNC Adult CI Program (~140 
surgeries/year) at initial activation (~2 -4 weeks postop). Inclusion criteria are 18 -65 years of age, MED -EL 
array recipient, and a postoperative unaided detection threshold of ≤65 dB HL at 125 Hz. Recruitment is limited 
to recip ients of one manufacturer to control for number and separation of electrod es, as well as coding 
strategy. Subjects will be fit with the Sonnet2EAS device. Subjects must identify English as their native 
language since test materials are in English. Subjects will be excluded if they report or present with a cognitive 
delay or impa irment, as indicated with the Mini Mental State Examination ( Folstein, Folstein, & McHugh; 1975 ). 
Subjects will be randomized into one of 2 groups : 1) default map, or 2) place -based map. In addition to these 
40 CI users, age -matched NH listeners will be re cruited for reference.  
 
Monaural speech perception will be compared between groups over the study period (activation, and 1, 3, 6, & 
12 mos). Subjects will be evaluated with the alternative map at the initial activation and 12 -mo intervals for 
within -subjects comparisons. The tester will be blinded to map condition to limit bias. Test measures include: 
CNC word recognition in quiet, AzBio sentence  recognition in noise (10 dB SNR), and vowel recognition. Word 
and sentence recognition will be assessed in the sound field, at one mete r from the speaker. Recorded 
materials will be presented at 60 dB SPL, and masking will be delivered to the contralateral ear via an insert 
phone when warranted. Vowel recognition is essential for speech perception and is sensitive to changes in 
low-to-mid frequency representation in EAS simulations. Vowel recognition will be assessed using the English 
Vowel Recognition Test from TigerSpeech Technology© , a tool that assesses recognition  of 12 vowel sounds, 
presented in a /h/ -vowel -/d/ context. The EAS devic e will be connected to the computer via a direct -connect 
cable. The subject  will listen to the target and select the perceived vowel from a closed -set list , with r esponses 
scored in percent correct ; confusion matrices are also provided . An age -matched , NH control group will  
complete the CNC and AzBio tests while listening to EAS simulations  of the default and place -based maps 
based on parameters characterizing stimulation for individual CI subjects; NH listeners are tested only once.   
 
Dependent measures are vowel recognition, word recognition in quiet, and sentence recognition in noise. 
Linear Mixed Models (LMM) will be used to assess the main effects of group (default vs place -based) and 
interval (activation, 1, 3, 6, & 12 mos), and th e interaction between group and interval on each measure, with a 
significance criterion of  =0.05. Aided acoustic audibility as measured with the Speech Intelligibility Index (SII) 
will be included as a within -subjects variable to control for differences i n residual hearing. Subject demographic 
data to be reported include: gender, age, unaided thresholds, AID of the most apical electrode, and mismatch. 
Mismatch will be quantified as the absolute semitone deviation at 1500 Hz (~267° using the SG function), 
which has been shown to be an important region for frequency alignment in vocoder simulations. Reduced 
models will assess the main effects of map (default vs place -based) and interval (activation & 12 mos), and the 
two-way interaction for the within -subject s comparison. Results from the NH control group will be compared to 
EAS users at activation, to corroborate effects of mapping approach in the findings from the CI subjects.  
 
For Aim 2, we plan to investigate the binaural hearing abilities of EAS users listening with a place -based map 
versus a default map. We plan to recruit 24 subjects from the experiments under Aim 1 with thresholds ≤65 dB 
HL at 125 Hz in the contralateral ear. For NH subjects listening to a bilateral CI simulation, SRM  is ~8 dB (95% 
CI: ~7 dB) with no interaural mismatch and ~4 dB (95% CI: ~11 dB) with an interaural mismatch of 1 mm (Xu 
et al., 2020). Similar results are obtained with bilateral CI recipients (Hu et al., 2018). Based on these data, our 
experiment is powered to observe effect sizes of d≥0.90 (α<0.05, 1 -β=0.8).  
 
Binaural hearing will be assessed in the soundbooth, using an 11-speaker arc spanning -90° to 90°. Subject 
listen with EAS+HA or EAS+NH at the 1, 3, 6, & 12 -month intervals. Binaural hearing measures include: 1) 
spatially masked sentence recognition, 2) localization, and 3) subjective benefit. Spatially masked senten ce 
recognition will be completed, with performance calculated as SRM EAS and SRM Contra. The dB SNR for each 
subject will be determined at the 1 -month interval in the co -located condition, by decreasing the dB SNR from 
10 dB to 0 dB in 5 dB steps until the s ubject scores <50% correct. Subjects will be evaluated at the same SNR 
for the subsequent intervals. Localization will be assessed by calculating the root -mean -squared (RMS) error 
of sound source identification using 200 -ms noise bursts at varied intensity  levels (52, 62, or 72 dB SPL). 
Subjective benefit will be assessed using the Speech, Spatial, and Qualities of Hearing Scale (SSQ; 
Gatehouse & Noble, 2004), with results evaluated on the 3 subscales and 10 pragmatic subscales (defined by 
Gatehouse & Akeroyd, 2006).   
 
The primary analysis is the effect of group (default vs place -based) on SRM EAS, SRM Contra, and localization. An 
LMM will assess the main effects ( group  and interval) and the associated interaction,  with a significance 
criterion of α<0.05 . Secondary analyses will include the effect of group on subjective benefit, assessed for the 
Spatial Hearing subscale, and relevant pragmatic subscales, including: 1) Speech in Speech Contexts, 2) 
Segregation of Sounds and Objects, and 3) Listening Effort.  
 
 